This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where empty lines have been removed, content has been compressed (code blocks are separated by ⋮---- delimiter).

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Empty lines have been removed from all files
- Content has been compressed - code blocks are separated by ⋮---- delimiter
- Files are sorted by Git change count (files with more changes are at the bottom)


================================================================
Directory Structure
================================================================
scripts/
  check-legacy-schema-urls.mjs
  dump-repo-for-ai.mjs
src/
  adapters/
    docx.ts
    index.ts
    pdf.ts
    rtf.ts
    rtfdZip.ts
    txt.ts
  lib/
    zip.ts
  pipeline/
    emit.ts
    extract.ts
    index.ts
    normalize.ts
    schema.ts
    segment.ts
    toSoustack.ts
    types.ts
    validate.ts
  types/
    external.d.ts
  cli.ts
test/
  fixtures/
    .gitkeep
    cinnamon-toast.txt
    sample.rtf
    structured-cookbook.txt
  cli.spec.ts
  docxPdfAdapters.spec.ts
  pipeline.spec.ts
  rtfdZip.spec.ts
.gitignore
AGENTS.md
LICENSE
package.json
README.md
tsconfig.json
tsconfig.test.json

================================================================
Files
================================================================

================
File: src/adapters/txt.ts
================
import { promises as fs } from "fs";
import { AdapterOutput } from "../pipeline";
export async function readTxt(filePath: string): Promise<AdapterOutput>

================
File: test/fixtures/.gitkeep
================


================
File: LICENSE
================
MIT License

Copyright (c) 2025 RichardHerold

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

================
File: scripts/check-legacy-schema-urls.mjs
================
/**
 * Guard script that fails if legacy schema URLs are found in src/ or test/ directories.
 * This ensures we don't accidentally reintroduce old schema URLs.
 */
⋮----
function getAllFiles(dir, fileList = [])
⋮----
function checkFile(filePath)
⋮----
function main()
⋮----
// Directory doesn't exist, skip

================
File: scripts/dump-repo-for-ai.mjs
================
function parseArgs(argv)
⋮----
function toPosixPath(filePath)
⋮----
function globToRegex(glob)
⋮----
function compilePatterns(patterns)
⋮----
function matchPattern(relPath, isDir, entryName, entry, patterns)
⋮----
async function readIgnoreFile(root, filename)
⋮----
function isBinaryExtension(filePath)
⋮----
function isUtf8(buffer)
⋮----
function getTimestamp(repoRoot)
⋮----
// ignore
⋮----
function getGitMetadata(repoRoot)
⋮----
async function collectFiles(repoRoot, options)
⋮----
async function walk(currentDir)
⋮----
function formatOutput(
⋮----
async function main()

================
File: src/adapters/rtf.ts
================
import { spawn } from "child_process";
import { promises as fs } from "fs";
import { AdapterOutput } from "../pipeline";
function convertRtfFallback(rtf: string): string
async function convertWithNode(rtf: string): Promise<string | null>
async function runCommand(command: string, args: string[], input?: string): Promise<string>
async function convertWithPandoc(rtfPath: string): Promise<string | null>
async function convertWithTextutil(rtfPath: string): Promise<string | null>
export async function convertRtfToText(rtfPath: string): Promise<string>
export async function readRtf(filePath: string): Promise<AdapterOutput>

================
File: src/lib/zip.ts
================
import { execFileSync } from "node:child_process";
import fs from "node:fs";
import os from "node:os";
import path from "node:path";
export type ZipEntry = {
  entryName: string;
  isDirectory: boolean;
  getData: () => Buffer;
};
function listPaths(root: string): string[]
⋮----
const walk = (dir: string) =>
⋮----
function createEntry(entryPath: string, root: string): ZipEntry
export class ZipArchive
⋮----
constructor(zipPath?: string)
addFile(entryName: string, data: Buffer): void
writeZip(targetPath: string): void
getEntries(): ZipEntry[]

================
File: src/pipeline/index.ts
================


================
File: src/pipeline/normalize.ts
================
import { Line, NormalizedText } from "./types";
export function normalize(input: string): NormalizedText

================
File: test/fixtures/cinnamon-toast.txt
================
CINNAMON TOAST AND BUTTER
Toast the white bread
Add soft butter onto toast
Sprinkle the cinnamon/sugar mix

================
File: test/fixtures/structured-cookbook.txt
================
CHICKEN PICCATTA
By:
Unknown
Ingredients:
Chicken breasts
½ cup flour

SPRING SALAD
Ingredients:
Lettuce

QUICK SOUP
Ingredients:
Water

EASY TOAST
Ingredients:
Bread

SWEET TREAT
Ingredients:
Sugar

================
File: test/docxPdfAdapters.spec.ts
================
import { promises as fs, createWriteStream } from "fs";
import os from "os";
import path from "path";
import assert from "node:assert/strict";
import { describe, it, beforeEach, afterEach } from "node:test";
import { Document, Packer, Paragraph, TextRun } from "docx";
import PDFDocument from "pdfkit";
import { loadInput } from "../src/adapters";
⋮----
// pdf-parse has known compatibility issues with PDFKit-generated PDFs
// Try to parse, but if it fails with XRef error, that's a known limitation
⋮----
// If it's the known XRef error, skip the test with a note
⋮----
// This is a known compatibility issue between PDFKit and pdf-parse
// The adapter works with real PDFs, but PDFKit-generated test PDFs
// have XRef table issues that pdf-parse cannot handle

================
File: AGENTS.md
================
# soustack-ingest — agends

This repo turns “random recipe files found in the wild” into **Soustack JSON**.

This file is a living **agenda + guide** for humans *and* coding agents working in this codebase. Keep it lightweight, keep it honest, and update it when reality changes.

---

## What this repo does

- **Input:** recipe sources (currently `.txt`, `.rtfd` bundles, `.rtfd.zip`).
- **Output:** `index.json` plus `recipes/<slug>.soustack.json` files.
- **Flow:** adapter → normalize → segment → extract → toSoustack → validate → emit.

If you’re lost, start with `src/cli.ts` (or the `ingest()` function inside it).

---

## Quick commands

```bash
npm install
npm test
npm run build

# Run ingest locally
npm run ingest -- <inputPath> --out <outDir>

# Known-good repro run (uses test fixtures)
npm run repro

================
File: tsconfig.test.json
================
{
  "extends": "./tsconfig.json",
  "include": ["src", "test"],
  "compilerOptions": {
    "types": ["node"]
  }
}

================
File: src/adapters/docx.ts
================
import { promises as fs } from "fs";
import mammoth from "mammoth";
import { AdapterOutput } from "../pipeline";
export async function readDocx(inputPath: string): Promise<AdapterOutput>

================
File: src/adapters/pdf.ts
================
import { promises as fs } from "fs";
import pdfParse from "pdf-parse";
import { AdapterOutput } from "../pipeline";
export async function readPdf(inputPath: string): Promise<AdapterOutput>

================
File: src/pipeline/emit.ts
================
import { promises as fs } from "fs";
import path from "path";
import { SoustackRecipe } from "./types";
function slugify(value: string): string
export async function emit(recipes: SoustackRecipe[], outDir: string): Promise<void>

================
File: test/rtfdZip.spec.ts
================
import { existsSync, promises as fs } from "fs";
import os from "os";
import path from "path";
import assert from "node:assert/strict";
import { describe, it } from "node:test";
import { ZipArchive } from "../src/lib/zip";
import { loadInput } from "../src/adapters";
import { emit } from "../src/pipeline/emit";
import { extract } from "../src/pipeline/extract";
import { normalize } from "../src/pipeline/normalize";
import { segment } from "../src/pipeline/segment";
import { toSoustack } from "../src/pipeline/toSoustack";
import { validate } from "../src/pipeline/validate";
import { SoustackRecipe } from "../src/pipeline/types";

================
File: tsconfig.json
================
{
  "compilerOptions": {
    "target": "ES2022",
    "module": "CommonJS",
    "moduleResolution": "node",
    "rootDir": "src",
    "outDir": "dist",
    "strict": true,
    "esModuleInterop": true,
    "forceConsistentCasingInFileNames": true,
    "skipLibCheck": true,
    "resolveJsonModule": true,
    "types": ["node"],
    "typeRoots": ["./node_modules/@types", "./src/types"]
  },
  "include": ["src"],
  "ts-node": {
    "esm": false
  }
}

================
File: src/pipeline/schema.ts
================


================
File: src/adapters/index.ts
================
import path from "path";
import { promises as fs } from "fs";
import { readRtfdZip, readRtfdDirectory } from "./rtfdZip";
import { readTxt } from "./txt";
import { readDocx } from "./docx";
import { readPdf } from "./pdf";
import { readRtf } from "./rtf";
import { AdapterOutput } from "../pipeline";
⋮----
export async function loadInput(inputPath: string): Promise<AdapterOutput>
⋮----
// Check if it's a directory (rtfd bundle) or a zip file
⋮----
// Treat as zip file

================
File: src/adapters/rtfdZip.ts
================
/// <reference path="../types/external.d.ts" />
import { promises as fs } from "fs";
import os from "os";
import path from "path";
import { AdapterOutput } from "../pipeline";
import { ZipArchive } from "../lib/zip";
import { convertRtfToText } from "./rtf";
type RtfCandidate = {
  filePath: string;
  size: number;
  priority: number;
};
async function listFiles(root: string): Promise<string[]>
function scoreRtfCandidate(filePath: string): number
async function findPrimaryRtf(extractedPath: string): Promise<RtfCandidate | null>
export async function readRtfdZip(filePath: string): Promise<AdapterOutput>
export async function readRtfdDirectory(dirPath: string): Promise<AdapterOutput>

================
File: src/types/external.d.ts
================


================
File: test/cli.spec.ts
================
import { afterEach, beforeEach, describe, it } from "node:test";
import assert from "node:assert/strict";
import { promises as fs } from "node:fs";
import os from "node:os";
import path from "node:path";
import { ingest } from "../src/cli";

================
File: .gitignore
================
soustack-injest-repo-pack.txt
/out
.DS_Store
**/.DS_Store

================
File: README.md
================
# soustack-ingest

## CLI usage

```bash
soustack-ingest ingest <inputPath> --out <outDir>
```

The CLI reads the input file, runs it through the ingest pipeline, and writes JSON outputs under `<outDir>` (see `src/cli.ts` and `src/pipeline/emit.ts`).

### Prerequisites

- Node.js 18+ (or compatible)
- Optional: `pandoc` for improved RTF/RTFD conversion (the adapter will fall back to a built-in parser when it is unavailable).

## Adapter behavior

Adapters are selected by file extension (`src/cli.ts`, `src/adapters`).

- `.rtfd.zip`: handled by `readRtfdZip` (`src/adapters/rtfdZip.ts`). The adapter extracts the archive, locates the primary `.rtf` payload (preferring `TXT.rtf` or the largest `.rtf` file), and converts it to text. It tries a Node-based parser first, then falls back to `pandoc` and `textutil` when available.
- `.txt`: handled by `readTxt` (`src/adapters/txt.ts`). Reads the file as UTF-8 text and passes it to the pipeline.
- `.docx`: handled by `readDocx` (`src/adapters/docx.ts`). Extracts plain text from Microsoft Word documents using `mammoth`.
- `.pdf`: handled by `readPdf` (`src/adapters/pdf.ts`). Extracts plain text from PDF files using `pdf-parse`.

Unsupported extensions throw an error.

## Pipeline stages & contracts

The ingest pipeline runs stages in order (`src/cli.ts`, `src/pipeline`).

1. **normalize** (`src/pipeline/normalize.ts`)
   - **Input:** raw adapter text (`string`).
   - **Output:** `NormalizedText` with `fullText` and line metadata (`Line[]`).
   - **Contract:** normalize newlines to `\n` and assign 1-based line numbers.

2. **segment** (`src/pipeline/segment.ts`)
   - **Input:** `Line[]`.
   - **Output:** `SegmentedText` with `Chunk[]`.
   - **Contract:** scores potential recipe boundaries and returns one chunk per inferred recipe with a best-effort title guess and confidence score.

3. **extract** (`src/pipeline/extract.ts`)
   - **Input:** a `Chunk` plus the full `Line[]`.
   - **Output:** `IntermediateRecipe` containing title, ingredients, instructions, and source-line evidence.
   - **Contract:** splits lines into `ingredients` and `instructions` sections by headers; lines before any header fall into instructions.

4. **toSoustack** (`src/pipeline/toSoustack.ts`)
   - **Input:** `IntermediateRecipe`.
   - **Output:** `SoustackRecipe` (Soustack JSON shape) with `$schema` (canonical URL), `profile: "lite"`, `stacks` as an object map, normalized `ingredients`/`instructions` string arrays, and ingest metadata.
   - **Contract:** embeds source path and line range into `metadata.ingest`.

5. **validate** (`src/pipeline/validate.ts`)
   - **Input:** `SoustackRecipe`.
   - **Output:** `ValidationResult` (`ok`, `errors`).
   - **Contract:** see validator notes below.

6. **emit** (`src/pipeline/emit.ts`)
   - **Input:** list of validated `SoustackRecipe` values and an output directory.
   - **Output:**
     - `<outDir>/index.json` with name/slug/path entries.
     - `<outDir>/recipes/<slug>.soustack.json` files for each recipe.
   - **Contract:** recipe filenames are slugified from `recipe.name` and truncated to 80 characters.

## Validator behavior & wiring `soustack`

Validation is intentionally lightweight today. The pipeline starts with a stub validator built from a fallback schema (`src/pipeline/validate.ts`). It attempts to load `soustack` at runtime:

- If `soustack` exports `validator`, that object is used.
- If it exports `validateRecipe`, it is wrapped into a `validator`.
- If neither exists or the import fails, the stub validator stays active.

To wire `soustack` validation:

1. Ensure `soustack` is installed (already in `package.json`).
2. Export either a `validator` object with a `validate(recipe)` function, or a `validateRecipe(recipe)` function, from the `soustack` package entry point.
3. Call `initValidator()` once at startup (the CLI does this before any `validate()` calls) so the active validator is set deterministically.

## Build, test, and run

```bash
npm run build
npm test
npm run ingest -- <inputPath> --out <outDir>
```

### Example usage

```bash
npm run ingest -- "/mnt/data/bowman cookbook.rtfd.zip" --out ./output
```

================
File: src/pipeline/validate.ts
================
import { createRequire } from "node:module";
import fs from "node:fs";
import path from "node:path";
import Ajv, { ErrorObject, type AnySchema } from "ajv";
import { SoustackRecipe, ValidationResult } from "./types";
export interface Validator {
  validate(recipe: SoustackRecipe): ValidationResult;
}
⋮----
validate(recipe: SoustackRecipe): ValidationResult;
⋮----
type CoreValidationModule = {
  validator?: Validator;
  validateRecipe?: (
    recipe: SoustackRecipe,
    options?: unknown
  ) =>
    | {
        ok?: boolean;
        valid?: boolean;
        success?: boolean;
        schemaErrors?: Array<{ path?: string; message?: string }>;
        conformanceIssues?: Array<{ path?: string; message?: string }>;
        errors?: Array<{ path?: string; message?: string } | string>;
        warnings?: string[];
      }
    | Promise<{
        ok?: boolean;
        valid?: boolean;
        success?: boolean;
        schemaErrors?: Array<{ path?: string; message?: string }>;
        conformanceIssues?: Array<{ path?: string; message?: string }>;
        errors?: Array<{ path?: string; message?: string } | string>;
        warnings?: string[];
      }>;
  recipeSchema?: unknown;
  schema?: unknown;
  schemas?: {
    recipe?: unknown;
  };
};
⋮----
// Accept canonical URL, legacy URLs, or missing (for backward compatibility)
// The canonical URL is preferred, but we don't hard-fail on legacy values
⋮----
function toJsonPathSegment(segment: string): string
function toJsonPath(instancePath: string, missingProperty?: string): string
function formatAjvError(error: ErrorObject): string
function buildAjvValidator(schema: unknown): Validator
function resolveSchemaFromModule(core: CoreValidationModule): unknown | null
async function loadSchemaFromString(schemaRef: string, moduleName: string): Promise<unknown | null>
async function loadSchemaFromPackage(moduleName: string): Promise<unknown | null>
function normalizeIssue(issue: unknown): string
function normalizeValidateRecipeResult(raw: unknown): ValidationResult
function wrapValidateRecipe(
  validateRecipeFn: NonNullable<CoreValidationModule["validateRecipe"]>
): Validator
async function selectValidatorFromModule(moduleName: string): Promise<Validator | null>
⋮----
export async function initValidator(): Promise<void>
export function validate(recipe: SoustackRecipe): ValidationResult

================
File: package.json
================
{
  "name": "@soustack/ingest",
  "version": "0.1.1",
  "publishConfig": {
    "access": "public"
  },
  "scripts": {
    "build": "tsc",
    "test": "node scripts/check-legacy-schema-urls.mjs && TS_NODE_PROJECT=tsconfig.test.json TS_NODE_TRANSPILE_ONLY=1 node -r ts-node/register --test test/*.spec.ts",
    "ingest": "ts-node src/cli.ts ingest",
    "repro": "npm run ingest -- test/fixtures/sample.rtf --out /tmp/soustack-ingest-repro"
  },
  "dependencies": {
    "adm-zip": "^0.5.10",
    "ajv": "^8.12.0",
    "commander": "^11.0.0",
    "fs-extra": "^11.2.0",
    "mammoth": "^1.6.0",
    "pdf-parse": "^1.1.1",
    "rtf2text": "^1.0.1",
    "slugify": "^1.6.6",
    "soustack": "^0.4.0",
    "zod": "^3.23.8"
  },
  "devDependencies": {
    "@types/node": "^25.0.3",
    "docx": "^8.5.0",
    "pdfkit": "^0.15.0",
    "ts-node": "^10.9.2",
    "typescript": "^5.4.5",
    "vitest": "^1.5.0"
  }
}

================
File: src/pipeline/segment.ts
================
import { Line, SegmentedText, Chunk, SegmentationReason } from "./types";
type LineFeatures = {
  isBlank: boolean;
  isTitleLike: boolean;
  isAllCapsTitle: boolean;
  hasIngredientsMarker: boolean;
  hasInstructionMarker: boolean;
  isIngredientLine: boolean;
  isImperativeLine: boolean;
};
⋮----
function isTitleLikeLine(
  text: string,
  prevBlank: boolean,
  nextBlank: boolean,
  index: number,
  totalLines: number,
): boolean
⋮----
// Require capitalization even when surrounded by blank lines or at edges.
// This avoids tagging short ingredient lines like "Chicken breasts" as titles.
⋮----
function isIngredientCandidate(text: string): boolean
function isAllCapsTitle(text: string): boolean
function isImperativeLine(text: string): boolean
function buildFeatures(lines: Line[]): LineFeatures[]
function clamp(value: number, min = 0, max = 1): number
function ingredientDensity(features: LineFeatures[], start: number, window: number): number
function imperativeDensity(features: LineFeatures[], start: number, window: number): number
type CandidateStart = {
  index: number;
  score: number;
  reason?: SegmentationReason;
};
function findCandidateStarts(lines: Line[], features: LineFeatures[]): CandidateStart[]
⋮----
// Filter out attribution/byline patterns (e.g., "From the former Boston Restaurant...")
⋮----
// Filter out section headers (e.g., "Ingredients", "Directions") - these are not recipe titles
⋮----
// Prefer the earlier candidate when they're close together (likely same recipe)
// Only replace if the later candidate has significantly higher score (>0.1 difference)
⋮----
function hasNearbyStructuredMarker(lines: Line[], startIndex: number, window = 8): boolean
function isAuthorLine(lines: Line[], index: number): boolean
function findStructuredCookbookStarts(lines: Line[], features: LineFeatures[]): number[]
function confidenceForChunk(
  lines: Line[],
  features: LineFeatures[],
  startIndex: number,
  endIndex: number,
): number
export type SegmentOptions = {
  debug?: boolean;
};
export function segment(lines: Line[], options: SegmentOptions =

================
File: src/pipeline/toSoustack.ts
================
import { SCHEMA_URL } from "./schema";
import { IntermediateRecipe, PrepMetadata, SoustackRecipe } from "./types";
⋮----
function toTitleCase(rawTitle: string): string
export function toSoustack(
  intermediate: IntermediateRecipe,
  options?: { sourcePath?: string }
): SoustackRecipe

================
File: src/cli.ts
================
import { Command } from "commander";
import {
  emit,
  extract,
  normalize,
  segment,
  toSoustack,
  initValidator,
  validate,
  SoustackRecipe,
  PrepExtractionMode,
} from "./pipeline";
import { loadInput } from "./adapters";
type IngestOptions = {
  debugSegmentation?: boolean;
  prepExtractionMode?: PrepExtractionMode;
};
export async function ingest(
  inputPath: string,
  outDir: string,
  options: IngestOptions = {},
): Promise<void>
⋮----
const recordSkip = (reason: string) =>
⋮----
function parsePrepExtractionMode(mode?: string): PrepExtractionMode

================
File: src/pipeline/types.ts
================
export type Line = { n: number; text: string };
export type NormalizedText = {
  fullText: string;
  lines: Line[];
};
export type Chunk = {
  startLine: number;
  endLine: number;
  titleGuess?: string;
  confidence: number;
  evidence: string;
  segmentationReason?: SegmentationReason;
};
export type SegmentedText = {
  chunks: Chunk[];
};
export type SegmentationReason =
  | "ingredient-density"
  | "imperative-density"
  | "all-caps-imperative";
export type IntermediateRecipe = {
  title: string;
  ingredients: string[];
  instructions: string[];
  instructionParagraphs: string[];
  prepSection?: string[];
  ingredientPrep?: IngredientPrep[];
  source: {
    startLine: number;
    endLine: number;
    evidence: string;
    author?: string;
  };
};
export type IngredientPrep = {
  index: number;
  raw: string;
  base: string;
  prep: string[];
};
export type PrepExtractionMode = "conservative" | "aggressive";
export type PrepMetadata = {
  section?: string[];
  ingredients?: IngredientPrep[];
  generatedAt?: string;
};
export type SoustackMetadata = Record<string, unknown> & {
  author?: string;
  instructionParagraphs?: string[];
  originalTitle?: string;
  ingest?: {
    pipelineVersion?: string;
    sourcePath?: string;
    sourceLines?: {
      start: number;
      end: number;
    };
    warnings?: string[];
  };
};
export type SoustackRecipe = {
  $schema: string;
  profile: "lite";
  name: string;
  stacks: Record<string, unknown>;
  ingredients: string[];
  instructions: string[];
  "x-prep"?: PrepMetadata;
  metadata?: SoustackMetadata;
};
export type ValidationResult = {
  ok: boolean;
  errors: string[];
};
export type AdapterOutput = {
  kind: "text";
  text: string;
  assets?: string[];
  meta: {
    sourcePath: string;
    extractedPath?: string;
  };
};

================
File: src/pipeline/extract.ts
================
import {
  Chunk,
  IntermediateRecipe,
  IngredientPrep,
  Line,
  PrepExtractionMode,
} from "./types";
function sliceLines(lines: Line[], startLine: number, endLine: number): Line[]
⋮----
function isAuthorNameLine(text: string): boolean
function extractAuthorFromLines(lines: Line[]):
function isPrepHeader(text: string): boolean
function splitSections(lines: Line[]):
⋮----
const normalizeHeader = (text: string)
const isIngredientHeader = (text: string)
const isInstructionHeader = (text: string)
const isByLine = (text: string)
⋮----
const cleanIngredient = (text: string)
const cleanInstruction = (text: string)
const cleanPrep = (text: string)
⋮----
const isIngredientLike = (text: string)
const isInstructionLike = (text: string)
const isClearInstructionLike = (text: string)
⋮----
const flushParagraph = () =>
const pushInstruction = (text: string, includeParagraph = true) =>
⋮----
function normalizePrepToken(token: string, mode: PrepExtractionMode): string | null
function extractIngredientPrep(
  raw: string,
  mode: PrepExtractionMode,
):
function extractLeadingPrepPhrase(
  text: string,
  mode: PrepExtractionMode,
):
function inferIngredientsFromInstructions(instructions: string[]): string[]
function isImperativeLine(text: string): boolean
function extractNounPhrasesFromImperative(text: string): string[]
function normalizeIngredientCandidate(segment: string): string | null
function dedupePreserveOrder(items: string[]): string[]
export function extract(
  chunk: Chunk,
  lines: Line[],
  options: { prepExtractionMode?: PrepExtractionMode } = {},
): IntermediateRecipe

================
File: test/pipeline.spec.ts
================
import { afterEach, beforeEach, describe, it } from "node:test";
import assert from "node:assert/strict";
import { promises as fs } from "fs";
import os from "os";
import path from "path";
import { normalize } from "../src/pipeline/normalize";
import { segment } from "../src/pipeline/segment";
import { extract } from "../src/pipeline/extract";
import { toSoustack } from "../src/pipeline/toSoustack";
import { emit } from "../src/pipeline/emit";
import { initValidator, validate } from "../src/pipeline/validate";
import { IntermediateRecipe, SoustackRecipe } from "../src/pipeline/types";





================================================================
End of Codebase
================================================================
